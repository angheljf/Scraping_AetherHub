{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import pandas as pd\n",
    "from tkinter import Tk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the LegenVD's deck list site\n",
    "url = 'https://aetherhub.com/User/LegenVD/Decks'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Beautiful Soup object\n",
    "html = browser.html\n",
    "decks_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of all the decks in the first page\n",
    "table = decks_soup.find('table', id='metaHubTable')\n",
    "table_body = table.find('tbody')\n",
    "table_rows = table_body.find_all('tr')\n",
    "# table_titles = table_rows[0].find_all('a')\n",
    "# table_titles[0].text\n",
    "\n",
    "for i in table_rows:\n",
    "    table_titles = i.find_all('a')\n",
    "    table_href = i.find_all('a', href=True)\n",
    "    print(table_titles[0].text)\n",
    "    print(table_href[0]['href'])\n",
    "    print('-------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the next page button and click it\n",
    "browser.links.find_by_partial_text('>').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a for loop to go through all the pages and get the names and links of all the decks\n",
    "for i in range(0, 1):\n",
    "    html = browser.html\n",
    "    table = decks_soup.find('table', id='metaHubTable')\n",
    "    table_body = table.find('tbody')\n",
    "    table_rows = table_body.find_all('tr')\n",
    "    time.sleep(5)\n",
    "    for i in table_rows:\n",
    "        table_titles = i.find_all('a')\n",
    "        table_href = i.find_all('a', href=True)\n",
    "        print(table_titles[0].text)\n",
    "        print(table_href[0]['href'])\n",
    "        print('-------------------')\n",
    "    time.sleep(5)\n",
    "    browser.links.find_by_partial_text('>').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same as above but update the decks_soup variable after each click\n",
    "for i in range(0, 2):\n",
    "    html = browser.html\n",
    "    decks_soup = soup(html, 'html.parser')\n",
    "    table = decks_soup.find('table', id='metaHubTable')\n",
    "    table_body = table.find('tbody')\n",
    "    table_rows = table_body.find_all('tr')\n",
    "    time.sleep(5)\n",
    "    for i in table_rows:\n",
    "        table_titles = i.find_all('a')\n",
    "        table_href = i.find_all('a', href=True)\n",
    "        print(table_titles[0].text)\n",
    "        print(table_href[0]['href'])\n",
    "        print('-------------------')\n",
    "    time.sleep(5)\n",
    "    browser.links.find_by_partial_text('>').click()\n",
    "    decks_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that does the above and adds the information to a dictionary\n",
    "# And saves the dictionary to a csv file\n",
    "def scrape_deck_info(deck_info, url, end_range):\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    browser.visit(url)\n",
    "    partial_url = 'https://aetherhub.com'\n",
    "    for i in range(0, end_range):\n",
    "        html = browser.html\n",
    "        decks_soup = soup(html, 'html.parser')\n",
    "        table = decks_soup.find('table', id='metaHubTable')\n",
    "        time.sleep(5)\n",
    "        table_body = table.find('tbody')\n",
    "        time.sleep(5)\n",
    "        table_rows = table_body.find_all('tr')\n",
    "        time.sleep(5)\n",
    "        for i in table_rows:\n",
    "            table_titles = i.find_all('a')\n",
    "            table_href = i.find_all('a', href=True)\n",
    "            # Find text with class 'text-muted'\n",
    "            table_format = i.find_all('span', class_='text-muted')\n",
    "            deck_info.append({'name': table_titles[0].text, 'link': partial_url + table_href[0]['href'], 'format': table_format[0].text})\n",
    "            # Check if there are duplicates in the link key inside the list of dictionaries\n",
    "            # If there are, stop the loop\n",
    "            if len(Counter([i['link'] for i in deck_info])) < len(deck_info):\n",
    "                break    \n",
    "        time.sleep(5)\n",
    "        # Click the '>' button, except for the last page, then stop the loop\n",
    "        try:\n",
    "            browser.links.find_by_partial_text('>').click()\n",
    "        except:\n",
    "            break\n",
    "        decks_soup = soup(html, 'html.parser')\n",
    "    deck_info_df = pd.DataFrame(deck_info)\n",
    "    # Remove quotes from the name column\n",
    "    deck_info_df['name'] = deck_info_df['name'].str.replace('\"', '')\n",
    "    deck_info_df.to_csv('deck_info.csv', index=False)\n",
    "    return deck_info[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck_info = []\n",
    "scrape_deck_info(deck_info, \"https://aetherhub.com/User/LegenVD/Decks\", 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the decklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>format</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training Grounds Transformers</td>\n",
       "      <td>https://aetherhub.com/Deck/training-grounds-tr...</td>\n",
       "      <td>Arena Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samut Haste Aggro</td>\n",
       "      <td>https://aetherhub.com/Deck/samut-haste-aggro</td>\n",
       "      <td>Arena Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multiverse Mill</td>\n",
       "      <td>https://aetherhub.com/Deck/multiverse-mill-912521</td>\n",
       "      <td>Arena Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Izzet Invasion</td>\n",
       "      <td>https://aetherhub.com/Deck/izzet-invasion</td>\n",
       "      <td>Arena Standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boros Flicker</td>\n",
       "      <td>https://aetherhub.com/Deck/boros-flicker-908904</td>\n",
       "      <td>Arena Standard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name   \n",
       "0  Training Grounds Transformers  \\\n",
       "1              Samut Haste Aggro   \n",
       "2                Multiverse Mill   \n",
       "3                 Izzet Invasion   \n",
       "4                  Boros Flicker   \n",
       "\n",
       "                                                link          format  \n",
       "0  https://aetherhub.com/Deck/training-grounds-tr...  Arena Standard  \n",
       "1       https://aetherhub.com/Deck/samut-haste-aggro  Arena Standard  \n",
       "2  https://aetherhub.com/Deck/multiverse-mill-912521  Arena Standard  \n",
       "3          https://aetherhub.com/Deck/izzet-invasion  Arena Standard  \n",
       "4    https://aetherhub.com/Deck/boros-flicker-908904  Arena Standard  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the csv file as a dataframe\n",
    "deck_info_df = pd.read_csv('deck_info.csv')\n",
    "deck_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the links to get the deck lists and save them as text files\n",
    "for i in deck_info_df['link'][0:2]:\n",
    "    browser.visit(i)\n",
    "    browser.links.find_by_partial_text('Export').click()\n",
    "    browser.links.find_by_partial_text('Text List').click()\n",
    "    # Click button with id \"exportListbtn\"\n",
    "    browser.find_by_id('exportListbtn').click()\n",
    "    text = Tk().clipboard_get()\n",
    "    text = text.split('\\n')\n",
    "    # Make the name of the file based on the 'name' column\n",
    "    file_name = i.split('/')[-1]\n",
    "    file_name = file_name.replace('-', '_')\n",
    "    file_name = file_name.replace(' ', '_')\n",
    "    with open(f'deck_lists/{file_name}.txt', 'w') as f:\n",
    "        for i in text:\n",
    "            f.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that does the above\n",
    "def scrape_deck_lists(deck_info_df, start_range: int):\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    for i in deck_info_df['link'][start_range:]:\n",
    "        time.sleep(3)\n",
    "        browser.visit(i)\n",
    "        browser.links.find_by_partial_text('Export').click()\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            browser.find_by_css('a[href=\"#importModal\"]').click()\n",
    "        except:\n",
    "            browser.links.find_by_partial_text('Text List').click()\n",
    "        time.sleep(3)\n",
    "        # Click button with id \"exportListbtn\"\n",
    "        browser.find_by_id('exportListbtn').click()\n",
    "        time.sleep(1)\n",
    "        text = Tk().clipboard_get()\n",
    "        text = text.split('\\n')\n",
    "        # Make the name of the file based on the 'name' column\n",
    "        file_name = i.split('/')[-1]\n",
    "        file_name = file_name.replace('-', '_')\n",
    "        file_name = file_name.replace(' ', '_')\n",
    "        with open(f'deck_lists/{file_name}.txt', 'w') as f:\n",
    "            for i in text:\n",
    "                f.write(i + '\\n')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.30M/6.30M [00:00<00:00, 19.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "scrape_deck_lists(deck_info_df, 774)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
